{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import time\n",
    "import random\n",
    "import gc\n",
    "\n",
    "from pandas import *\n",
    "\n",
    "# SK-learn libraries.\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.svm import SVC\n",
    "import re\n",
    "\n",
    "# scipy\n",
    "from scipy.signal import butter, lfilter\n",
    "\n",
    "\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import and Merge The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Change the 'path' variable to run on your machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1438904986.26 Merging...\n",
      "1438904986.26 C:\\Users\\marks\\Google Drive\\EEG Kaggle\\train\\subj1_series1_data.csv\n",
      "1438904987.52 C:\\Users\\marks\\Google Drive\\EEG Kaggle\\train\\subj1_series2_data.csv\n",
      "Merge Complete: 2.75499987602 total seconds\n",
      "1438904989.02 Merging...\n",
      "1438904989.02 C:\\Users\\marks\\Google Drive\\EEG Kaggle\\train\\subj1_series1_events.csv\n",
      "1438904989.2 C:\\Users\\marks\\Google Drive\\EEG Kaggle\\train\\subj1_series2_events.csv\n",
      "Merge Complete: 0.513000011444 total seconds\n",
      "1438904989.53 Merging...\n",
      "1438904989.53 C:\\Users\\marks\\Google Drive\\EEG Kaggle\\train\\subj1_series3_data.csv\n",
      "Merge Complete: 1.2650001049 total seconds\n",
      "1438904990.8 Merging...\n",
      "1438904990.8 C:\\Users\\marks\\Google Drive\\EEG Kaggle\\train\\subj1_series3_events.csv\n",
      "Merge Complete: 0.256000041962 total seconds\n"
     ]
    }
   ],
   "source": [
    "#Remove the channels we don't want \n",
    "def Remove_Channels(df):\n",
    "#     df.drop(df.columns[[15,16,20,21,22,25,26,27,28,29,30,31,32]], axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "path =r'C:\\Users\\marks\\Google Drive\\EEG Kaggle\\train'\n",
    "train_data_filenames = glob.glob(path + \"/subj1_series[1-2]*data.csv\") #load only subject 1, series 1-3\n",
    "list_ = []\n",
    "Train_Array_Lengths = []\n",
    "Start_Time = time.time()\n",
    "print time.time(), \"Merging...\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for file_ in train_data_filenames:\n",
    "    print time.time(), file_\n",
    "    df = pd.read_csv(file_,index_col=None, header=0,usecols =list(range(1, 33, 1)),dtype = 'float16')\n",
    "    list_.append(Remove_Channels(df))\n",
    "    Train_Array_Lengths.append(len(df))\n",
    "Train_Data = pd.concat(list_)\n",
    "\n",
    "print \"Merge Complete:\", time.time()-Start_Time, \"total seconds\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#now with event data\n",
    "path =r'C:\\Users\\marks\\Google Drive\\EEG Kaggle\\train'\n",
    "train_event_filenames = glob.glob(path + \"/subj1_series[1-2]*events.csv\") #load only subject 1, series 1-3\n",
    "list_ = []\n",
    "\n",
    "Start_Time = time.time()\n",
    "print time.time(), \"Merging...\"\n",
    "\n",
    "for file_ in train_event_filenames:\n",
    "    print time.time(), file_\n",
    "    df = pd.read_csv(file_,index_col=None, header=0)\n",
    "    list_.append(df)\n",
    "train_events = pd.concat(list_)\n",
    "\n",
    "print \"Merge Complete:\", time.time()-Start_Time, \"total seconds\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#do the same thing and now get your testing data\n",
    "test_data_filenames = glob.glob(path + \"/subj1_series3*data.csv\") #load only subject 1, series 1-3\n",
    "list_ = []\n",
    "Test_Array_Lengths = []\n",
    "Start_Time = time.time()\n",
    "print time.time(), \"Merging...\"\n",
    "\n",
    "for file_ in test_data_filenames:\n",
    "    print time.time(), file_\n",
    "    df = pd.read_csv(file_,index_col=None, header=0,usecols =list(range(1, 33, 1)),dtype = 'float16')\n",
    "    list_.append(Remove_Channels(df))\n",
    "    Test_Array_Lengths.append(len(df))\n",
    "Test_Data = pd.concat(list_)\n",
    "\n",
    "print \"Merge Complete:\", time.time()-Start_Time, \"total seconds\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#now with event data\n",
    "path =r'C:\\Users\\marks\\Google Drive\\EEG Kaggle\\train'\n",
    "test_event_filenames = glob.glob(path + \"/subj1_series3*events.csv\") #load only subject 1, series 1-3\n",
    "list_ = []\n",
    "\n",
    "Start_Time = time.time()\n",
    "print time.time(), \"Merging...\"\n",
    "\n",
    "for file_ in test_event_filenames:\n",
    "    print time.time(), file_\n",
    "    df = pd.read_csv(file_,index_col=None, header=0)\n",
    "    list_.append(df)\n",
    "test_events = pd.concat(list_)\n",
    "\n",
    "print \"Merge Complete:\", time.time()-Start_Time, \"total seconds\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(391450, 32)\n",
      "Fp1      3266\n",
      "Fp2      3048\n",
      "F7       1735\n",
      "F3       1388\n",
      "Fz        801\n",
      "F4       2416\n",
      "F8       5112\n",
      "FC5      3204\n",
      "FC1       545\n",
      "FC2       665\n",
      "FC6      9008\n",
      "T7       4844\n",
      "C3        969\n",
      "Cz        376\n",
      "C4       2158\n",
      "T8      11312\n",
      "TP9      8320\n",
      "CP5      2039\n",
      "CP1       815\n",
      "CP2       907\n",
      "CP6      1736\n",
      "TP10     4184\n",
      "P7       2088\n",
      "P3        818\n",
      "Pz       1140\n",
      "P4        884\n",
      "P8       1400\n",
      "PO9      1611\n",
      "O1       1272\n",
      "Oz        949\n",
      "O2        978\n",
      "PO10     1372\n",
      "dtype: float16\n",
      "[[ -82  -78 -127 -128 -128 -127 -128]\n",
      " [ 127  127  127 -124  127 -114 -109]\n",
      " [-128 -128  127  127  127  127  127]]\n"
     ]
    }
   ],
   "source": [
    "print train_Data.shape\n",
    "print np.max(Train_Data,axis=0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "test_data = np.vstack(([1.413,2.1423,3.254,4.3451,5.124,6.634,7.124],[8,9,10,11,12,13,14],[0,.5,10,511,12,130,100]))\n",
    "\n",
    "\n",
    "print Scale_array_Smaller(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separate the data into the 6 events and into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Train_Labels_HandStart =  train_events['HandStart'].to_sparse(fill_value=0)\n",
    "Train_Labels_FirstDigitTouch =  train_events['FirstDigitTouch'].to_sparse(fill_value=0)\n",
    "Train_Labels_BothStartLoadPhase =  train_events['BothStartLoadPhase'].to_sparse(fill_value=0)\n",
    "Train_Labels_LiftOff =  train_events['LiftOff'].to_sparse(fill_value=0)\n",
    "Train_Labels_Replace =  train_events['Replace'].to_sparse(fill_value=0)\n",
    "Train_Labels_BothReleased =  train_events['BothReleased'].to_sparse(fill_value=0)\n",
    "# Train_Labels_Combined = train_events.HandStart.map(str) + ',' + train_events.FirstDigitTouch.map(str) + ',' + train_events.BothStartLoadPhase.map(str) + ',' + train_events.LiftOff.map(str) + ',' + train_events.Replace.map(str) + ',' + train_events.BothReleased.map(str)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Test_Labels_HandStart =  test_events['HandStart'].to_sparse(fill_value=0)\n",
    "Test_Labels_FirstDigitTouch =  test_events['FirstDigitTouch'].to_sparse(fill_value=0)\n",
    "Test_Labels_BothStartLoadPhase =  test_events['BothStartLoadPhase'].to_sparse(fill_value=0)\n",
    "Test_Labels_LiftOff =  test_events['LiftOff'].to_sparse(fill_value=0)\n",
    "Test_Labels_Replace =  test_events['Replace'].to_sparse(fill_value=0)\n",
    "Test_Labels_BothReleased =  test_events['BothReleased'].to_sparse(fill_value=0)\n",
    "\n",
    "\n",
    "\n",
    "#delete what isn't needed anymore to save memory. \n",
    "del train_events\n",
    "del test_events\n",
    "gc.collect() #Garbage collection (i.e. get rid of any outstanding unused memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define functions for feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#bin the time \n",
    "def Bin_Time(num_rows,num_bins):\n",
    "    Bin_Size = num_rows/num_bins\n",
    "    Bins = np.zeros(shape=(num_rows,num_bins))\n",
    "    Bin_Min = 0\n",
    "    Bin_Max = Bin_Size\n",
    "    for i in range(0,num_bins):\n",
    "        Bins[Bin_Min:Bin_Max,i] = 1\n",
    "        Bin_Min = Bin_Min + Bin_Size\n",
    "        Bin_Max = Bin_Max + Bin_Size\n",
    "    return Bins\n",
    "\n",
    "\n",
    "##filters borrowed from Nihar. \n",
    "def butter_bandpass(lowcut, highcut, fs, order=5):\n",
    "    nyq = 0.5 * fs\n",
    "    low = lowcut / nyq\n",
    "    high = highcut / nyq\n",
    "    b, a = butter(order, [low, high], btype='band')\n",
    "    return b, a\n",
    "\n",
    "def butter_bandpass_filter(data, lowcut, highcut, fs, order=5):\n",
    "    b, a = butter_bandpass(lowcut, highcut, fs, order=order)\n",
    "    y = lfilter(b, a, data)\n",
    "    return y\n",
    "\n",
    "\n",
    "#run PCA and return the number of PCs that explain the given amount of variance. \n",
    "def extractFeatures_PCA(Train_Data,Test_Data, PercentVarExplained):\n",
    "    pca = PCA()\n",
    "    pca.fit(Train_Data)  \n",
    "    \n",
    "    Explained_Variance_Ratios = pca.explained_variance_ratio_\n",
    "    for i in range(1,len(Explained_Variance_Ratios)):\n",
    "        if sum(Explained_Variance_Ratios[0:i]) >= PercentVarExplained:\n",
    "                   NumPCs = i + 1 #add 1 since numpy array ranges are not inclusive\n",
    "                   break\n",
    "    PCA_Projections = pca.transform(Train_Data)[:,0:NumPCs]\n",
    "    PCA_Projections_Test = pca.transform(Test_Data)[:,0:NumPCs]\n",
    "    return PCA_Projections,PCA_Projections_Test\n",
    "\n",
    "\n",
    "\n",
    "#return rolling mean of each column in a pandas dataframe with a given window. Returns df of same size. \n",
    "def df_rolling_mean(df,window):\n",
    "    list_ = []\n",
    "    Num_Cols = len(df.columns)\n",
    "    for i in range(0,Num_Cols):\n",
    "        Roll_Array = pd.rolling_mean(df.iloc[0:,i],window,min_periods = 0).fillna(0)\n",
    "        list_.append(Roll_Array)\n",
    "    new_df = pd.concat(list_,1)\n",
    "    for i in range(0,Num_Cols):\n",
    "        new_df=new_df.rename(columns = {i:'mean' + str(i)})\n",
    "    return np.float16(new_df)\n",
    "\n",
    "\n",
    "\n",
    "#return rolling variance of each column in a pandas dataframe with a given window. Returns df of same size. \n",
    "def df_rolling_var(df,window):\n",
    "    list_ = []\n",
    "    Num_Cols = len(df.columns)\n",
    "    for i in range(0,Num_Cols):\n",
    "        Roll_Array = pd.rolling_var(df.iloc[0:,i],window,min_periods = 0).fillna(0)\n",
    "        list_.append(Roll_Array)\n",
    "    new_df = pd.concat(list_,1)\n",
    "    for i in range(0,Num_Cols):\n",
    "        new_df=new_df.rename(columns = {i:'var' + str(i)})\n",
    "    return np.float16(new_df)\n",
    "\n",
    "\n",
    "#return rolling min of each column in a pandas dataframe with a given window. Returns df of same size. \n",
    "def df_rolling_min(df,window):\n",
    "    list_ = []\n",
    "    Num_Cols = len(df.columns)\n",
    "    for i in range(0,Num_Cols):\n",
    "        Roll_Array = pd.rolling_min(df.iloc[0:,i],window,min_periods = 0).fillna(0)\n",
    "        list_.append(Roll_Array)\n",
    "    new_df = pd.concat(list_,1)\n",
    "    for i in range(0,Num_Cols):\n",
    "        new_df=new_df.rename(columns = {i:'min' + str(i)})\n",
    "    return np.float16(new_df)\n",
    "\n",
    "\n",
    "#return rolling max of each column in a pandas dataframe with a given window. Returns df of same size. \n",
    "def df_rolling_max(df,window):\n",
    "    list_ = []\n",
    "    Num_Cols = len(df.columns)\n",
    "    for i in range(0,Num_Cols):\n",
    "        Roll_Array = pd.rolling_max(df.iloc[0:,i],window,min_periods = 0).fillna(0)\n",
    "        list_.append(Roll_Array)\n",
    "    new_df = pd.concat(list_,1)\n",
    "    for i in range(0,Num_Cols):\n",
    "        new_df=new_df.rename(columns = {i:'max' + str(i)})\n",
    "    return np.float16(new_df)\n",
    "\n",
    "\n",
    "\n",
    "#return rolling skew of each column in a pandas dataframe with a given window. Returns df of same size. \n",
    "def df_rolling_skew(df,window):\n",
    "    list_ = []\n",
    "    Num_Cols = len(df.columns)\n",
    "    for i in range(0,Num_Cols):\n",
    "        Roll_Array = pd.rolling_skew(df.iloc[0:,i],window,min_periods = 0).fillna(0)\n",
    "        list_.append(Roll_Array)\n",
    "    new_df = pd.concat(list_,1)\n",
    "    for i in range(0,Num_Cols):\n",
    "        new_df=new_df.rename(columns = {i:'skew' + str(i)})\n",
    "    return np.float16(new_df)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#return rolling quantile of each column in a pandas dataframe with a given window and quantile. Returns df of same size. \n",
    "def df_rolling_quantile(df,window,quantile):\n",
    "    list_ = []\n",
    "    Num_Cols = len(df.columns)\n",
    "    for i in range(0,Num_Cols):\n",
    "        Roll_Array = pd.rolling_quantile(df.iloc[0:,i],window,quantile,min_periods = 0).fillna(0)\n",
    "        list_.append(Roll_Array)\n",
    "    new_df = pd.concat(list_,1)\n",
    "    for i in range(0,Num_Cols):\n",
    "        new_df=new_df.rename(columns = {i:'Pct'+ str(quantile) + \"_\" + str(i)})\n",
    "    return np.float16(new_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#Feature Creation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section takes rolling statistics for different windows of time for each column of the dataframe and combines them into a single multi-dimensional array. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculations With Window of 10 Complete:  27  Seconds\n",
      "Calculations With Window of 100 Complete:  55  Seconds\n",
      "Calculations With Window of 500 Complete:  81  Seconds\n",
      "Calculations With Window of 700 Complete:  109  Seconds\n",
      "544 Total Resultant Features\n"
     ]
    }
   ],
   "source": [
    "Windows = [10,100,500,700]\n",
    "\n",
    "Test_Features = []\n",
    "Train_Features = []\n",
    "\n",
    "Start_Time = time.time()\n",
    "for w in Windows:\n",
    "    Raw_Data_Rolling_Window = w\n",
    "    \n",
    "    \n",
    "    RawData_Rolling_Means_Train = df_rolling_mean(pd.DataFrame(Train_Data),Raw_Data_Rolling_Window)\n",
    "    RawData_Rolling_Skewness_Train = df_rolling_skew(pd.DataFrame(Train_Data),Raw_Data_Rolling_Window)\n",
    "    RawData_Rolling_Min_Train = df_rolling_min(pd.DataFrame(Train_Data),Raw_Data_Rolling_Window)\n",
    "    RawData_Rolling_Max_Train = df_rolling_max(pd.DataFrame(Train_Data),Raw_Data_Rolling_Window)\n",
    "\n",
    "\n",
    "    RawData_Rolling_Means_Test = df_rolling_mean(pd.DataFrame(Test_Data),Raw_Data_Rolling_Window)\n",
    "    RawData_Rolling_Skewness_Test = df_rolling_skew(pd.DataFrame(Test_Data),Raw_Data_Rolling_Window)\n",
    "    RawData_Rolling_Min_Test = df_rolling_min(pd.DataFrame(Test_Data),Raw_Data_Rolling_Window)\n",
    "    RawData_Rolling_Max_Test = df_rolling_max(pd.DataFrame(Test_Data),Raw_Data_Rolling_Window)\n",
    "\n",
    "    \n",
    "    print \"Calculations With Window of\", w, \"Complete: \", int(time.time()-Start_Time), \" Seconds\"\n",
    "\n",
    "    \n",
    "    #Append all the features into 1\n",
    "    Train_Features.append(np.concatenate((RawData_Rolling_Means_Train,RawData_Rolling_Skewness_Train,RawData_Rolling_Min_Train,RawData_Rolling_Max_Train),axis = 1))\n",
    "    Test_Features.append(np.concatenate((RawData_Rolling_Means_Test,RawData_Rolling_Skewness_Test,RawData_Rolling_Min_Test,RawData_Rolling_Max_Test),axis = 1))\n",
    "    \n",
    "\n",
    "del RawData_Rolling_Means_Train\n",
    "del RawData_Rolling_Means_Test\n",
    "\n",
    "del RawData_Rolling_Skewness_Train\n",
    "del RawData_Rolling_Skewness_Test\n",
    "\n",
    "del RawData_Rolling_Min_Train\n",
    "del RawData_Rolling_Min_Test\n",
    "\n",
    "del RawData_Rolling_Max_Train\n",
    "del RawData_Rolling_Max_Test\n",
    "\n",
    "gc.collect() #Garbage collection (i.e. get rid of any outstanding unused memory)\n",
    "\n",
    "Test_Features = np.concatenate((Test_Features),axis = 1)\n",
    "gc.collect() \n",
    "\n",
    "Train_Features =np.concatenate((Train_Features),axis = 1)\n",
    "gc.collect()\n",
    "\n",
    "Test_Features = np.concatenate((Test_Features,Test_Data),axis = 1)\n",
    "gc.collect()\n",
    "\n",
    "Train_Features =np.concatenate((Train_Features,Train_Data),axis = 1)\n",
    "\n",
    "del Train_Data\n",
    "del Test_Data\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "print Train_Features.shape[1], \"Total Resultant Features\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Add additional features here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Nihar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inf\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'PCA' object has no attribute 'explained_variance_ratio_'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-250-c8018b0d367e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maverage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTrain_Features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m \u001b[0mExplained_Variance_Ratios\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpca\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexplained_variance_ratio_\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[0mExplained_Variance_Ratios\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[0mNumPCs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'PCA' object has no attribute 'explained_variance_ratio_'"
     ]
    }
   ],
   "source": [
    "#run PCA and return the number of PCs that explain the given amount of variance. \n",
    "#set the percent of the total variance explained we want explained by our subset of principal components. \n",
    "Pct_Variance_Explained = .93\n",
    "\n",
    "\n",
    "Start_Time = time.time() # begin timer\n",
    "\n",
    "# we must first scale the data. \n",
    "Scale_Center = StandardScaler(copy=False)\n",
    "Z_Train_Features = Scale_Center.fit_transform(np.array(Train_Features))\n",
    "gc.collect()  #Garbage collection (i.e. get rid of any outstanding unused memory)\n",
    "\n",
    "\n",
    "Z_Test_Features = Scale_Center.fit_transform(np.array(Test_Features))\n",
    " \n",
    "gc.collect()\n",
    "\n",
    "\n",
    "\n",
    "pca = PCA()\n",
    "print np.average(Train_Features)\n",
    "    \n",
    "Explained_Variance_Ratios = pca.explained_variance_ratio_\n",
    "print Explained_Variance_Ratios\n",
    "NumPCs = 1\n",
    "for i in range(1,len(Explained_Variance_Ratios)):\n",
    "    print i\n",
    "    if sum(Explained_Variance_Ratios[0:i]) >= PercentVarExplained:\n",
    "        NumPCs = i + 1 #add 1 since numpy array ranges are not inclusive\n",
    "        break\n",
    "PCA_Projections = pca.transform(Train_Data)[:,0:NumPCs]\n",
    "PCA_Projections_Test = pca.transform(Test_Data)[:,0:NumPCs]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#return Scale_array_Smaller(PCA_Projections),Scale_array_Smaller(PCA_Projections_Test)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Jeff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -31.      363.      211.     ...,  173.      120.      704.    ]\n",
      " [ -30.      352.5     213.5    ...,  141.       83.      737.    ]\n",
      " [ -77.3125  327.75    177.375  ...,  141.       62.      677.    ]\n",
      " ..., \n",
      " [  86.      257.25    220.25   ...,  109.       -7.      118.    ]\n",
      " [  92.      249.75    227.5    ...,  128.       47.      184.    ]\n",
      " [ 108.375   246.875   232.75   ...,  156.       66.      245.    ]]\n",
      "[[  16.       274.       243.      ...,  -56.       -35.       167.     ]\n",
      " [  21.       264.5      254.5     ...,  -94.       -88.        96.     ]\n",
      " [  35.65625  245.       269.75    ...,  -40.       -35.       148.     ]\n",
      " ..., \n",
      " [ 209.75     240.5      426.75    ...,  112.        88.       287.     ]\n",
      " [ 215.25     247.375    459.25    ...,  105.        52.       184.     ]\n",
      " [ 212.25     264.       485.25    ...,  103.        42.       152.     ]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print Z_Train_Features\n",
    "print Test_Features\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Carson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Ji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Principal Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reduce the dimensionality of our feature set before modeling begins. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float16').",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-244-f69e07450a58>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[0mPCs_Train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mPCs_Test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextractFeatures_PCA\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mZ_Train_Features\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mZ_Test_Features\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mPct_Variance_Explained\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;32mdel\u001b[0m \u001b[0mZ_Train_Features\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mZ_Test_Features\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[0mgc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-242-0b94cf13a25f>\u001b[0m in \u001b[0;36mextractFeatures_PCA\u001b[1;34m(Train_Data, Test_Data, PercentVarExplained)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mextractFeatures_PCA\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTrain_Data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mTest_Data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mPercentVarExplained\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[0mpca\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPCA\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m     \u001b[0mpca\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTrain_Data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[0mExplained_Variance_Ratios\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpca\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexplained_variance_ratio_\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\marks\\AppData\\Local\\Continuum\\Anaconda\\lib\\site-packages\\sklearn\\decomposition\\pca.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    223\u001b[0m             \u001b[0mReturns\u001b[0m \u001b[0mthe\u001b[0m \u001b[0minstance\u001b[0m \u001b[0mitself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m         \"\"\"\n\u001b[1;32m--> 225\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    226\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\marks\\AppData\\Local\\Continuum\\Anaconda\\lib\\site-packages\\sklearn\\decomposition\\pca.pyc\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    267\u001b[0m             \u001b[0mrequested\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    268\u001b[0m         \"\"\"\n\u001b[1;32m--> 269\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    270\u001b[0m         \u001b[0mn_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    271\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mas_float_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\marks\\AppData\\Local\\Continuum\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.pyc\u001b[0m in \u001b[0;36marray2d\u001b[1;34m(X, dtype, order, copy, force_all_finite)\u001b[0m\n\u001b[0;32m    120\u001b[0m     \u001b[0mX_2d\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0matleast_2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 122\u001b[1;33m         \u001b[0m_assert_all_finite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_2d\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    123\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mX\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mX_2d\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m         \u001b[0mX_2d\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msafe_copy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_2d\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\marks\\AppData\\Local\\Continuum\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.pyc\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[1;34m(X)\u001b[0m\n\u001b[0;32m     41\u001b[0m             and not np.isfinite(X).all()):\n\u001b[0;32m     42\u001b[0m         raise ValueError(\"Input contains NaN, infinity\"\n\u001b[1;32m---> 43\u001b[1;33m                          \" or a value too large for %r.\" % X.dtype)\n\u001b[0m\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float16')."
     ]
    }
   ],
   "source": [
    "#set the percent of the total variance explained we want explained by our subset of principal components. \n",
    "Pct_Variance_Explained = .93\n",
    "\n",
    "\n",
    "Start_Time = time.time() # begin timer\n",
    "\n",
    "# we must first scale the data. \n",
    "Scale_Center = StandardScaler()\n",
    "Z_Train_Features = Scale_Center.fit_transform(np.array(Train_Features))\n",
    "del Train_Features \n",
    "gc.collect()  #Garbage collection (i.e. get rid of any outstanding unused memory)\n",
    "\n",
    "\n",
    "Z_Test_Features = Scale_Center.fit_transform(np.array(Test_Features))\n",
    "del Test_Features \n",
    "gc.collect()\n",
    "\n",
    "\n",
    "PCs_Train,PCs_Test = extractFeatures_PCA(Z_Train_Features,Z_Test_Features,Pct_Variance_Explained)\n",
    "del Z_Train_Features,Z_Test_Features \n",
    "gc.collect()\n",
    "print \"PCA Complete:\", int(time.time()-Start_Time), \" Seconds\"\n",
    "\n",
    "print ''\n",
    "print PCs_Train.shape[1], \"Resultant Principal Components\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print np.mean(Train_Features,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Different Models\n",
    "\n",
    "##Logistic Regession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "#set parameters for Logistic Regression\n",
    "#Setting them here will allow the optimization of the parameters. We saw in project 2 how much the C-value can affect the results. \n",
    "C_Value = 1\n",
    "penalty = 'l2'\n",
    "Convergence_tol = .001\n",
    "\n",
    "#Create a function that trains and runs a logistic regression model. Then prints and returns the AUC score\n",
    "def predictCategory_LogReg(train_data, train_label, test_data, test_label, Category, C_Value,penalty, Convergence_tol):\n",
    "    Logistic_Reg = LogisticRegression(C = C_Value, penalty = penalty,tol=Convergence_tol)    \n",
    "    Logistic_Reg.fit(train_data, train_label)\n",
    "    print(Category, \" Score =\", Logistic_Reg.score(test_data, test_label))\n",
    "\n",
    "    prob = Logistic_Reg.predict_proba(test_data)\n",
    "    print(Category, \"AUC\", roc_auc_score(test_label, prob[:,1]))\n",
    "\n",
    "    #FPR, TPR, thresholds = roc_curve(Test_Labels_HandStart, prob[:,1])\n",
    "    return roc_auc_score(test_label, prob[:,1])\n",
    "\n",
    "\n",
    "Start_Time = time.time()   \n",
    "AUC_HandStart = predictCategory_LogReg(PCs_Train,Train_Labels_HandStart.to_dense(),PCs_Test,Test_Labels_HandStart.to_dense(), 'HandStart', C_Value,penalty, Convergence_tol)\n",
    "AUC_FirstDigitTouch = predictCategory_LogReg(PCs_Train,Train_Labels_FirstDigitTouch.to_dense(),PCs_Test,Test_Labels_FirstDigitTouch.to_dense(), 'FirstDigitTouch', C_Value,penalty, Convergence_tol)\n",
    "AUC_BothStartLoadPhase = predictCategory_LogReg(PCs_Train,Train_Labels_BothStartLoadPhase.to_dense(),PCs_Test,Test_Labels_BothStartLoadPhase.to_dense(), 'BothStartLoadPhase', C_Value,penalty, Convergence_tol)\n",
    "AUC_LiftOff = predictCategory_LogReg(PCs_Train,Train_Labels_LiftOff.to_dense(),PCs_Test,Test_Labels_LiftOff.to_dense(), 'LiftOff', C_Value,penalty, Convergence_tol)\n",
    "AUC_Replace = predictCategory_LogReg(PCs_Train,Train_Labels_Replace.to_dense(),PCs_Test,Test_Labels_Replace.to_dense(), 'Replace', C_Value,penalty, Convergence_tol)\n",
    "AUC_BothReleased = predictCategory_LogReg(PCs_Train,Train_Labels_BothReleased.to_dense(),PCs_Test,Test_Labels_BothReleased.to_dense(), 'BothReleased', C_Value,penalty, Convergence_tol)\n",
    "print int(time.time()-Start_Time), \" Seconds to complete\"\n",
    "print \"Overall Logistic Regression Score = \", np.mean((AUC_HandStart, AUC_FirstDigitTouch,AUC_BothStartLoadPhase,AUC_LiftOff,AUC_Replace,AUC_BothReleased))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is set up to get started. It has not been tested. MM 08/04/2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #set parameters for SVM\n",
    "# #Setting them here will allow the optimization of the parameters. We saw in project 2 how much the C-value can affect the results. \n",
    "# C_Value = 1\n",
    "# kernel = 'rbf'\n",
    "# degree = 3\n",
    "\n",
    "# #Create a function that trains and runs a model. Then prints and returns the AUC score\n",
    "# def predictCategory_SVM(train_data, train_label, test_data, test_label, Category, C_Value,kernel, degree):\n",
    "#     SVM = SVC(C = C_Value, kernel = kernel,degree=degree)    \n",
    "#     SVC.fit(train_data, train_label)\n",
    "\n",
    "#     prob = SVC.predict_proba(test_data)\n",
    "#     print(Category, \"AUC\", roc_auc_score(test_label, prob[:,1]))\n",
    "\n",
    "#     #FPR, TPR, thresholds = roc_curve(Test_Labels_HandStart, prob[:,1])\n",
    "#     return roc_auc_score(test_label, prob[:,1])\n",
    "    \n",
    "# AUC_HandStart = predictCategory_SVM(Feature_Array_Train,Train_Labels_HandStart,Feature_Array_Test,Test_Labels_HandStart, 'HandStart', Category, C_Value,kernel, degree)\n",
    "# AUC_FirstDigitTouch = predictCategory_SVM(Feature_Array_Train,Train_Labels_FirstDigitTouch,Feature_Array_Test,Test_Labels_FirstDigitTouch, 'FirstDigitTouch', Category, C_Value,kernel, degree)\n",
    "# AUC_BothStartLoadPhase = predictCategory_SVM(Feature_Array_Train,Train_Labels_BothStartLoadPhase,Feature_Array_Test,Test_Labels_BothStartLoadPhase, 'BothStartLoadPhase', Category, C_Value,kernel, degree)\n",
    "# AUC_LiftOff = predictCategory_SVM(Feature_Array_Train,Train_Labels_LiftOff,Feature_Array_Test,Test_Labels_LiftOff, 'LiftOff', Category, C_Value,kernel, degree)\n",
    "# AUC_Replace = predictCategory_SVM(Feature_Array_Train,Train_Labels_Replace,Feature_Array_Test,Test_Labels_Replace, 'Replace', Category, C_Value,kernel, degree)\n",
    "# AUC_BothReleased = predictCategory_SVM(Feature_Array_Train,Train_Labels_BothReleased,Feature_Array_Test,Test_Labels_BothReleased, 'BothReleased', Category, C_Value,kernel, degree\n",
    "\n",
    "\n",
    "# print \"Overall SVM Score = \", np.mean(AUC_HandStart, AUC_FirstDigitTouch,AUC_BothStartLoadPhase,AUC_LiftOff,AUC_Replace,AUC_BothReleased)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is set up to get started. It has not been tested. MM 08/04/2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #set parameters for Neural Network model\n",
    "# #Setting them here will allow the optimization of the parameters. We saw in project 2 how much the C-value can affect the results. \n",
    "# n_components = 10\n",
    "# learning_rate = .1\n",
    "# batch_size = 20000\n",
    "# n_iter =1\n",
    "\n",
    "\n",
    "# #Create a function that trains and runs a model. Then prints and returns the AUC score\n",
    "# def predictCategory_Neural_Network(train_data, train_label, test_data, test_label, Category, C_Value,kernel, degree):\n",
    "#     SVM = SVC(C = C_Value, kernel = kernel,degree=degree)    \n",
    "#     SVC.fit(train_data, train_label)\n",
    "\n",
    "#     prob = SVC.predict_proba(test_data)\n",
    "#     print(Category, \"AUC\", roc_auc_score(test_label, prob[:,1]))\n",
    "\n",
    "#     #FPR, TPR, thresholds = roc_curve(Test_Labels_HandStart, prob[:,1])\n",
    "#     return roc_auc_score(test_label, prob[:,1])\n",
    "    \n",
    "# AUC_HandStart = predictCategory_Neural_Network(Feature_Array_Train,Train_Labels_HandStart,Feature_Array_Test,Test_Labels_HandStart, 'HandStart', Category, C_Value,kernel, degree)\n",
    "# AUC_FirstDigitTouch = predictCategory_Neural_Network(Feature_Array_Train,Train_Labels_FirstDigitTouch,Feature_Array_Test,Test_Labels_FirstDigitTouch, 'FirstDigitTouch', Category, C_Value,kernel, degree)\n",
    "# AUC_BothStartLoadPhase = predictCategory_Neural_Network(Feature_Array_Train,Train_Labels_BothStartLoadPhase,Feature_Array_Test,Test_Labels_BothStartLoadPhase, 'BothStartLoadPhase', Category, C_Value,kernel, degree)\n",
    "# AUC_LiftOff = predictCategory_Neural_Network(Feature_Array_Train,Train_Labels_LiftOff,Feature_Array_Test,Test_Labels_LiftOff, 'LiftOff', Category, C_Value,kernel, degree)\n",
    "# AUC_Replace = predictCategory_Neural_Network(Feature_Array_Train,Train_Labels_Replace,Feature_Array_Test,Test_Labels_Replace, 'Replace', Category, C_Value,kernel, degree)\n",
    "# AUC_BothReleased = predictCategory_Neural_Network(Feature_Array_Train,Train_Labels_BothReleased,Feature_Array_Test,Test_Labels_BothReleased, 'BothReleased', Category, C_Value,kernel, degree\n",
    "\n",
    "\n",
    "# print \"Overall Neural Network Score = \", np.mean(AUC_HandStart, AUC_FirstDigitTouch,AUC_BothStartLoadPhase,AUC_LiftOff,AUC_Replace,AUC_BothReleased)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees and Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
